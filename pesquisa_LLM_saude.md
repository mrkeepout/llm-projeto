# Pesquisa Profunda: Sistema LLM para Responder Perguntas sobre Sa√∫de P√∫blica

**Disciplina:** Projeto e An√°lise de Algoritmos (PAA)  
**Tema:** Quest√µes de Sa√∫de P√∫blica  
**Data:** Novembro 2025

---

## 1. INTRODU√á√ÉO E CONTEXTO

### 1.1 Objetivo do Projeto
Desenvolver um sistema de Processamento de Linguagem Natural (NLP) capaz de responder perguntas em linguagem natural sobre sa√∫de p√∫blica, integrando:
- **Coleta e processamento de dados** sobre sa√∫de p√∫blica
- **Treinamento de modelo LLM** (Large Language Model) especializado
- **Interface web** interativa para intera√ß√£o do usu√°rio
- **An√°lise de complexidade** dos algoritmos implementados

### 1.2 Relev√¢ncia do Tema
Sa√∫de p√∫blica √© um dom√≠nio cr√≠tico onde:
- H√° demanda constante por informa√ß√µes precisas
- A precis√£o √© essencial (quest√µes m√©dicas)
- Existem conjuntos de dados p√∫blicos dispon√≠veis
- √â um contexto realista e impactante para demonstra√ß√£o em sala de aula

---

## 2. FUNDAMENTA√á√ÉO TE√ìRICA

### 2.1 O que √© um LLM (Large Language Model)?

Um **LLM** √© uma rede neural profunda baseada em arquitetura **Transformer** que foi pr√©-treinada em quantidades massivas de texto para:
- Compreender linguagem natural
- Gerar texto coerente
- Realizar transfer√™ncia de conhecimento para tarefas espec√≠ficas

**Caracter√≠sticas principais:**
- **Bilh√µes de par√¢metros** (pesos da rede)
- **Pr√©-treinamento** em corpus gen√©rico de texto
- **Fine-tuning** em dados espec√≠ficos do dom√≠nio
- **Arquitetura Transformer** com mecanismo de aten√ß√£o

### 2.2 Arquitetura Transformer

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INPUT (Pergunta do usu√°rio)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Tokeniza√ß√£o ‚îÇ (Converte texto em tokens)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Embedding + Position ‚îÇ (Representa√ß√£o vetorial)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Multi-Head     ‚îÇ
    ‚îÇ  Self-Attention ‚îÇ (Entende rela√ß√µes entre palavras)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Feed Forward   ‚îÇ (Processa representa√ß√µes)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Decoder Layers ‚îÇ (M√∫ltiplas camadas)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Linear + SoftMax ‚îÇ (Gera probabilidades de tokens)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ OUTPUT      ‚îÇ (Resposta em linguagem natural)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Componentes-chave:**
1. **Self-Attention:** Cada palavra "v√™" todas as outras, calculando relev√¢ncia
2. **Multi-Head:** M√∫ltiplas representa√ß√µes simult√¢neas
3. **Layers:** Processamento em camadas sucessivas
4. **Tokeniza√ß√£o:** Quebra texto em unidades process√°veis

### 2.3 Processo de Treinamento

#### 2.3.1 Pr√©-Treinamento (Feito pelo fabricante)
- Treinado em **centenas de bilh√µes de tokens** de texto gen√©rico
- Objetivo: Prever pr√≥ximo token na sequ√™ncia
- Resultado: Modelo compreende padr√µes gerais de linguagem

#### 2.3.2 Fine-Tuning (Seu projeto!)
- Pega modelo pr√©-treinado
- Treina com dados espec√≠ficos de **sa√∫de p√∫blica**
- Adapta pesos da rede para dom√≠nio espec√≠fico
- Melhora muito a qualidade das respostas

#### 2.3.3 Metodologias de Fine-Tuning

**A) Supervised Fine-Tuning (SFT)**
- Dataset: pares (pergunta, resposta esperada)
- Ensina modelo a gerar respostas espec√≠ficas
- Melhor para: classifica√ß√£o, respostas diretas
- Complexidade: O(n √ó m) onde n = tokens, m = exemplos

```python
# Pseudoc√≥digo SFT
for epoch in range(num_epochs):
    for batch in training_data:
        pergunta, resposta_esperada = batch
        logits = model(pergunta)
        loss = cross_entropy(logits, resposta_esperada)
        loss.backward()
        optimizer.step()
```

**B) Direct Preference Optimization (DPO)**
- Dataset: perguntas com respostas "boas" vs "ruins"
- Treina modelo a **preferir** respostas melhores
- Melhor para: racioc√≠nio cl√≠nico, triage
- **Recomendado para seu projeto** (mais efetivo)

```python
# Pseudoc√≥digo DPO
for batch in training_data:
    pergunta = batch['pergunta']
    resposta_boa = batch['resposta_boa']
    resposta_ruim = batch['resposta_ruim']
    
    score_boa = model.score(pergunta, resposta_boa)
    score_ruim = model.score(pergunta, resposta_ruim)
    
    loss = -log(sigmoid(score_boa - score_ruim))
    loss.backward()
```

**C) LoRA (Low-Rank Adaptation)** - RECOMENDADO PARA SEU CASO
- N√£o modifica todos os pesos
- Adiciona **adaptadores de baixa classifica√ß√£o**
- Reduz tempo de treinamento em **90%**
- Reduz mem√≥ria necess√°ria em **80%**
- Mant√©m qualidade do modelo original

```
Par√¢metros do modelo original: 7 bilh√µes
Par√¢metros adicionais LoRA: ~13 milh√µes (0.2%)
```

---

## 3. STACK TECNOL√ìGICO RECOMENDADO

### 3.1 Arquitetura Geral do Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            INTERFACE WEB (Frontend)                   ‚îÇ
‚îÇ  Framework: React.js / Vue.js / HTML5 + JavaScript    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ HTTP/WSGI
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SERVIDOR BACKEND (API REST/FastAPI)           ‚îÇ
‚îÇ  - Recebe pergunta em JSON                            ‚îÇ
‚îÇ  - Processa com modelo LLM                            ‚îÇ
‚îÇ  - Retorna resposta em JSON                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Carregamento em mem√≥ria
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PIPELINE DE PROCESSAMENTO                     ‚îÇ
‚îÇ  1. Tokeniza√ß√£o (HuggingFace Tokenizers)              ‚îÇ
‚îÇ  2. Embedding (Modelo)                               ‚îÇ
‚îÇ  3. Gera√ß√£o de tokens (Modelo)                        ‚îÇ
‚îÇ  4. Detokeniza√ß√£o                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MODELO LLM FINE-TUNED (Arquivo .pth/.safetensors)   ‚îÇ
‚îÇ  - Base: LLaMA 2 7B ou Mistral 7B                     ‚îÇ
‚îÇ  - Adaptadores LoRA aplicados                        ‚îÇ
‚îÇ  - Treinado em dados de sa√∫de p√∫blica               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Stack Espec√≠fico (Com Licen√ßas Open Source)

| Componente | Tecnologia | Licen√ßa | Justificativa |
|-----------|-----------|---------|---------------|
| **Linguagem Principal** | Python 3.10+ | PSF | Melhor ecossistema ML/NLP |
| **Framework Deep Learning** | PyTorch 2.0+ | BSD | Preferido para NLP, flex√≠vel |
| **Bibliotecas LLM** | Hugging Face Transformers | Apache 2.0 | Padr√£o ouro em NLP |
| **Fine-tuning Eficiente** | PEFT (LoRA) | Apache 2.0 | Implementa√ß√£o LoRA oficial |
| **Tokeniza√ß√£o** | HuggingFace Tokenizers | Apache 2.0 | Tokeniza√ß√£o otimizada |
| **Servidor Backend** | FastAPI | MIT | Moderno, perform√°tico, f√°cil |
| **Servidor WSGI** | Uvicorn | BSD | Servidor ASGI para FastAPI |
| **Frontend** | React.js ou HTML5 | MIT/Apache | Interface interativa |
| **Database (opcional)** | SQLite3 ou PostgreSQL | Public Domain/PostgreSQL | Persist√™ncia de dados |
| **Logging/Monitoramento** | Python logging | PSF | J√° na stdlib |
| **Containeriza√ß√£o** | Docker | Apache 2.0 | Reprodutibilidade |

### 3.3 Ambiente de Desenvolvimento

```bash
# Arquivo: requirements.txt
torch>=2.0.0          # PyTorch
transformers>=4.35.0  # Hugging Face
peft>=0.7.0           # LoRA
pydantic>=2.0         # Valida√ß√£o de dados
fastapi>=0.104.0      # Framework web
uvicorn>=0.24.0       # Servidor ASGI
numpy>=1.24.0         # Arrays
scikit-learn>=1.3.0   # Utilit√°rios ML
pandas>=2.0.0         # Manipula√ß√£o de dados
requests>=2.31.0      # Requisi√ß√µes HTTP
python-dotenv>=1.0.0  # Vari√°veis de ambiente
```

---

## 4. DATASET E COLETA DE DADOS

### 4.1 Fontes de Dados Recomendadas (P√∫blicas e Gratuitas)

#### **Op√ß√£o 1: WHO (Organiza√ß√£o Mundial de Sa√∫de)**
- **URL:** https://www.who.int/publications
- **Conte√∫do:** Relat√≥rios, guias, FAQ sobre doen√ßas
- **Formato:** PDF, HTML
- **Pr√©-processamento:** OCR + parsing

#### **Op√ß√£o 2: Datasets M√©dicos em Portugu√™s**
- **MedQA Dataset** (modificado para portugu√™s)
- **Perguntas e respostas m√©dicas** de comunidades
- **Documentos SUS** (Sistema √önico de Sa√∫de)

#### **Op√ß√£o 3: PubMed Central**
- **URL:** https://www.ncbi.nlm.nih.gov/pmc/
- **Conte√∫do:** Artigos cient√≠ficos de acesso livre
- **API:** Dispon√≠vel para download autom√°tico

#### **Op√ß√£o 4: Criar Seu Pr√≥prio Dataset**
- Manualmente com especialistas
- Crowdsourcing na universidade
- Perguntas frequentes de sa√∫de p√∫blica

### 4.2 Estrutura do Dataset

```json
{
  "training_data": [
    {
      "id": 1,
      "pergunta": "O que √© dengue?",
      "resposta": "Dengue √© uma doen√ßa infecciosa causada pelo v√≠rus...",
      "categoria": "doen√ßas_infecciosas",
      "confian√ßa": 0.95
    },
    {
      "id": 2,
      "pergunta": "Como se transmite o v√≠rus Zika?",
      "resposta": "O v√≠rus Zika √© transmitido principalmente pelo mosquito Aedes aegypti...",
      "categoria": "transmiss√£o_doen√ßa",
      "confian√ßa": 0.98
    }
  ]
}
```

### 4.3 Requisitos de Qualidade

- **M√≠nimo 500-1000 exemplos** para fine-tuning b√°sico
- **M√°ximo 10.000 exemplos** antes de overfitting
- **Balanceamento:** Distribui√ß√£o uniforme de categorias
- **Validade:** Revis√£o por especialista (se poss√≠vel)

### 4.4 Pr√©-Processamento de Dados

```python
# Pseudoc√≥digo de pipeline de dados
def preprocessar_dataset(raw_data):
    """
    Complexidade: O(n √ó m) onde:
    - n = n√∫mero de exemplos
    - m = comprimento m√©dio do texto
    """
    processado = []
    
    for exemplo in raw_data:
        # 1. Limpeza
        texto = remover_html(exemplo['text'])
        texto = remover_caracteres_especiais(texto)
        
        # 2. Normaliza√ß√£o
        texto = texto.lower()
        texto = remover_acentos(texto)
        
        # 3. Valida√ß√£o
        if len(texto) > 50 and len(texto) < 5000:
            processado.append({
                'pergunta': exemplo['pergunta'],
                'resposta': texto,
                'tokens_count': len(tokenizer.encode(texto))
            })
    
    return processado
```

---

## 5. METODOLOGIA DE TREINAMENTO

### 5.1 Escolha do Modelo Base

#### **LLaMA 2 7B (Recomendado)**
- **Tamanho:** 7 bilh√µes de par√¢metros
- **Licen√ßa:** Llama 2 Community License (permitida para fins educacionais)
- **Vantagens:**
  - Bom equil√≠brio entre qualidade e tamanho
  - Execut√°vel em GPU modesta (12GB VRAM)
  - Bem documentado
  - Excelente para portugu√™s
  
#### **Mistral 7B (Alternativa)**
- **Tamanho:** 7 bilh√µes de par√¢metros
- **Licen√ßa:** Apache 2.0
- **Vantagens:**
  - Performance superior ao LLaMA em certos benchmarks
  - Mais eficiente em termos de infer√™ncia

#### **Por que NOT usar GPT-4?**
- Propriet√°rio (n√£o open source)
- Requer API com custo
- Violaria requisito de "open source" do projeto

### 5.2 Configura√ß√£o de Fine-Tuning (LoRA)

```python
# Configura√ß√£o recomendada para seu projeto
lora_config = {
    "r": 16,                      # Rank dos adaptadores LoRA
    "lora_alpha": 32,             # Escala de aprendizado
    "target_modules": [
        "q_proj",                 # Query projection
        "v_proj"                  # Value projection
    ],
    "lora_dropout": 0.05,         # Dropout no LoRA
    "bias": "none",
    "task_type": "CAUSAL_LM"      # Linguagem causal (predi√ß√£o de pr√≥ximo token)
}

training_config = {
    "num_epochs": 3,              # 3 √©pocas (pode variar)
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 2e-4,        # Taxa de aprendizado
    "warmup_steps": 100,
    "max_steps": -1,              # Usa num_epochs
    "logging_steps": 10,
    "save_strategy": "epoch",
    "eval_strategy": "epoch",
    "weight_decay": 0.01,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "max_grad_norm": 1.0
}
```

### 5.3 Pipeline de Treinamento

```python
# Pseudoc√≥digo do pipeline
class LLMHealthTrainer:
    def __init__(self, model_name, config):
        """
        Complexidade de inicializa√ß√£o: O(1)
        - Carrega modelo pr√©-treinado
        """
        self.model = load_pretrained_model(model_name)
        self.tokenizer = load_tokenizer(model_name)
        self.config = config
    
    def preparar_dados(self, dataset_path):
        """
        Complexidade: O(n) onde n = tamanho do dataset
        """
        dataset = carregar_json(dataset_path)
        dataset = dataset.map(self.tokenizar_exemplo)
        dataset = dataset.filter(lambda x: len(x['input_ids']) < 2048)
        return dataset.train_test_split(test_size=0.1)
    
    def tokenizar_exemplo(self, exemplo):
        """
        Complexidade por exemplo: O(m) onde m = comprimento do texto
        """
        prompt = f"Pergunta: {exemplo['pergunta']}\nResposta: {exemplo['resposta']}"
        encoded = self.tokenizer.encode(prompt, max_length=2048, truncation=True)
        return {
            'input_ids': encoded,
            'attention_mask': [1] * len(encoded),
            'labels': encoded  # Mesmo que input para LM
        }
    
    def treinar(self, train_dataset, eval_dataset):
        """
        Complexidade: O(num_epochs √ó len(dataset) √ó modelo_forward_pass)
        T√≠pico: ~20-60 minutos em GPU moderna
        """
        trainer = Trainer(
            model=self.model,
            args=TrainingArguments(**self.config),
            train_dataset=train_dataset,
            eval_dataset=eval_dataset
        )
        
        trainer.train()
        trainer.save_model("./modelo_saude_publica_final")
    
    def avaliar(self, eval_dataset):
        """
        Complexidade: O(len(eval_dataset) √ó modelo_inference)
        """
        metricas = self.trainer.evaluate(eval_dataset)
        
        # Calcula BLEU, ROUGE, BERTScore
        return {
            'loss': metricas['eval_loss'],
            'perplexidade': math.exp(metricas['eval_loss'])
        }
```

---

## 6. AN√ÅLISE DE COMPLEXIDADE DOS ALGORITMOS

### 6.1 Tokeniza√ß√£o

**Algoritmo:** BPE (Byte-Pair Encoding)

```
Complexidade: O(n √ó log k)
onde:
  n = n√∫mero de caracteres
  k = tamanho do vocabul√°rio

Espa√ßo: O(v)
onde:
  v = tamanho do vocabul√°rio (~50.000 tokens)
```

### 6.2 Self-Attention (Transformer)

**F√≥rmula:** Attention(Q, K, V) = softmax(Q √ó K^T / ‚àöd_k) √ó V

```
Complexidade por head: O(n¬≤)
onde:
  n = comprimento da sequ√™ncia

Com multi-head (h heads):
Complexidade total: O(h √ó n¬≤) = O(n¬≤) assintoticamente

Para n=2048 tokens:
- Sem otimiza√ß√µes: 4.194.304 opera√ß√µes
- Com flash-attention: ~50% mais r√°pido
```

**Problema: Quadr√°tico!**
- Texto de 100 tokens: 10.000 opera√ß√µes
- Texto de 1.000 tokens: 1.000.000 opera√ß√µes
- Texto de 2.000 tokens: 4.000.000 opera√ß√µes

**Solu√ß√£o implementada:** Flash-Attention v2
- Reduz acesso √† mem√≥ria
- Mant√©m O(n¬≤) assintoticamente mas com constante menor

### 6.3 Fine-Tuning com LoRA

```
Sem LoRA (atualizar todos os pesos):
Complexidade: O(n_params √ó n_exemplos √ó n_tokens)
Mem√≥ria: O(n_params) = 7B √ó 4 bytes = 28GB

Com LoRA:
Complexidade: O((r √ó d) √ó n_exemplos √ó n_tokens)
onde:
  r = rank LoRA = 16
  d = dimens√£o = 4096
  
Mem√≥ria: O(r √ó d) = 16 √ó 4096 √ó 4 bytes = 262KB
Redu√ß√£o: 7B ‚Üí 13M (0.2% dos pesos)

Speedup: ~10x mais r√°pido
```

### 6.4 Infer√™ncia (Responder Pergunta)

```
Gera√ß√£o autoregressiva:
Complexity: O(n_max √ó d¬≤)
onde:
  n_max = comprimento m√°ximo de resposta
  d = dimens√£o hidden = 4096

Para resposta de 500 tokens:
- 500 forward passes
- Cada pass: ~1 bilh√£o de opera√ß√µes
- Total: ~500 bilh√µes de opera√ß√µes

Em GPU V100: ~5-10 segundos por resposta
Em GPU A100: ~2-3 segundos por resposta
```

### 6.5 Recupera√ß√£o de Informa√ß√£o (RAG - Opcional)

Se implementar **Retrieval-Augmented Generation:**

```
1. Embedding da pergunta: O(n_tokens)
2. Busca similar em KB: O(log k) com √≠ndice HNSW
3. Concatenar contexto: O(contexto_size)
4. Gerar resposta: O(n_resposta √ó d¬≤)

Complexidade total: O(log k + n √ó d¬≤)
Espa√ßo: O(k √ó d) onde k = documentos na KB
```

---

## 7. ARQUITETURA DO SISTEMA COMPLETO

### 7.1 Backend (FastAPI)

```python
# arquivo: main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

app = FastAPI(title="Assistente de Sa√∫de P√∫blica")

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)

# Carregar modelo uma √∫nica vez (na inicializa√ß√£o)
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "meta-llama/Llama-2-7b-hf"
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
)

# Carregar LoRA adapters
model = PeftModel.from_pretrained(
    base_model,
    "./modelo_saude_publica_final"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

class PerguntaRequest(BaseModel):
    pergunta: str
    max_tokens: int = 512

class RespostaResponse(BaseModel):
    pergunta: str
    resposta: str
    tempo_processamento: float

@app.post("/api/responder", response_model=RespostaResponse)
async def responder(request: PerguntaRequest):
    """
    Complexidade: O(n √ó d¬≤)
    onde n = tokens gerados, d = dimens√£o do modelo
    """
    start_time = time.time()
    
    # Tokenizar pergunta
    input_ids = tokenizer.encode(
        request.pergunta,
        return_tensors="pt"
    ).to(device)
    
    # Gerar resposta (O(n √ó d¬≤))
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=request.max_tokens,
            temperature=0.7,
            top_p=0.95,
            do_sample=True
        )
    
    # Detokenizar
    resposta = tokenizer.decode(
        output_ids[0],
        skip_special_tokens=True
    )
    
    tempo = time.time() - start_time
    
    return RespostaResponse(
        pergunta=request.pergunta,
        resposta=resposta,
        tempo_processamento=tempo
    )

@app.get("/api/saude")
async def informacoes():
    return {
        "status": "online",
        "modelo": "LLaMA 2 7B fine-tuned",
        "dispositivo": device
    }
```

### 7.2 Frontend (HTML + JavaScript)

```html
<!-- arquivo: index.html -->
<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assistente de Sa√∫de P√∫blica</title>
    <style>
        * { box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 0;
            padding: 20px;
        }
        
        .container {
            background: white;
            border-radius: 10px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
            max-width: 700px;
            width: 100%;
            padding: 30px;
        }
        
        h1 {
            color: #333;
            text-align: center;
            margin-bottom: 30px;
        }
        
        .chat-box {
            height: 400px;
            border: 2px solid #ddd;
            border-radius: 8px;
            padding: 15px;
            overflow-y: auto;
            background: #f9f9f9;
            margin-bottom: 20px;
        }
        
        .message {
            margin: 10px 0;
            padding: 10px 15px;
            border-radius: 8px;
            line-height: 1.5;
        }
        
        .user-message {
            background: #667eea;
            color: white;
            margin-left: 20px;
            text-align: right;
        }
        
        .bot-message {
            background: #e9ecef;
            color: #333;
            margin-right: 20px;
        }
        
        .input-group {
            display: flex;
            gap: 10px;
        }
        
        input {
            flex: 1;
            padding: 12px 15px;
            border: 2px solid #ddd;
            border-radius: 8px;
            font-size: 1em;
            transition: border-color 0.3s;
        }
        
        input:focus {
            outline: none;
            border-color: #667eea;
        }
        
        button {
            padding: 12px 30px;
            background: #667eea;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-weight: bold;
            transition: background 0.3s;
        }
        
        button:hover {
            background: #764ba2;
        }
        
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        
        .loading {
            text-align: center;
            color: #999;
            font-style: italic;
            padding: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üè• Assistente de Sa√∫de P√∫blica</h1>
        <div class="chat-box" id="chatBox"></div>
        <div class="input-group">
            <input 
                type="text" 
                id="perguntaInput" 
                placeholder="Fa√ßa uma pergunta sobre sa√∫de p√∫blica..."
                onkeypress="if(event.key === 'Enter') enviarPergunta()"
            />
            <button onclick="enviarPergunta()" id="enviarBtn">Enviar</button>
        </div>
    </div>

    <script>
        const API_URL = "http://localhost:8000";
        const chatBox = document.getElementById("chatBox");
        const perguntaInput = document.getElementById("perguntaInput");
        const enviarBtn = document.getElementById("enviarBtn");

        async function enviarPergunta() {
            const pergunta = perguntaInput.value.trim();
            if (!pergunta) return;

            // Adicionar mensagem do usu√°rio
            adicionarMensagem(pergunta, "user");
            perguntaInput.value = "";
            enviarBtn.disabled = true;

            // Mostrar "carregando"
            adicionarMensagem("‚è≥ Processando...", "loading");

            try {
                const response = await fetch(`${API_URL}/api/responder`, {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        pergunta: pergunta,
                        max_tokens: 512
                    })
                });

                if (!response.ok) throw new Error("Erro na resposta");

                const data = await response.json();
                
                // Remover "carregando"
                const loadingMsgs = chatBox.querySelectorAll('.loading');
                loadingMsgs.forEach(msg => msg.remove());

                // Adicionar resposta do bot
                adicionarMensagem(
                    data.resposta + `\n\n‚è±Ô∏è ${data.tempo_processamento.toFixed(2)}s`,
                    "bot"
                );
            } catch (error) {
                console.error(error);
                adicionarMensagem("‚ùå Erro ao processar pergunta", "bot");
            } finally {
                enviarBtn.disabled = false;
                perguntaInput.focus();
            }
        }

        function adicionarMensagem(texto, tipo) {
            const msgDiv = document.createElement("div");
            msgDiv.className = `message ${tipo}-message`;
            msgDiv.textContent = texto;
            chatBox.appendChild(msgDiv);
            chatBox.scrollTop = chatBox.scrollHeight;
        }
    </script>
</body>
</html>
```

---

## 8. O QUE APRESENTAR PARA A TURMA

### 8.1 Apresenta√ß√£o T√©cnica (Slides)

**Estrutura recomendada:**

1. **Introdu√ß√£o (2 min)**
   - O que √© sa√∫de p√∫blica
   - Por que usar LLMs
   - Demonstra√ß√£o r√°pida: fazer uma pergunta

2. **Dados (3 min)**
   - Fonte dos dados
   - Tamanho do dataset
   - Exemplos de perguntas/respostas
   - Gr√°fico de distribui√ß√£o de categorias

3. **Arquitetura (5 min)**
   - Diagrama do sistema
   - Fluxo de dados
   - Tecnologias utilizadas
   - Justificativa das escolhas

4. **Transformer (5 min)**
   - Explicar sucintamente como funciona
   - Self-attention com exemplo visual
   - Por que √© efetivo para linguagem natural

5. **Fine-Tuning (4 min)**
   - O que √© transfer learning
   - Como funciona LoRA
   - Compara√ß√£o: tempo e mem√≥ria
   - Gr√°fico de loss durante treinamento

6. **An√°lise de Complexidade (5 min)**
   - Tokeniza√ß√£o: O(n √ó log k)
   - Self-attention: O(n¬≤)
   - Fine-tuning: O(r √ó d √ó exemplos √ó tokens)
   - Compara√ß√£o: com/sem LoRA
   - Gr√°fico: tempo vs tamanho de entrada

7. **Resultados (4 min)**
   - M√©trica de acur√°cia (se tiver dados de teste)
   - Exemplos de perguntas respondidas
   - Tempo de resposta
   - Casos de sucesso e limita√ß√µes

8. **Demo ao Vivo (5 min)**
   - Abrir interface web
   - Fazer 3-4 perguntas diferentes
   - Mostrar tempo de processamento

---

### 8.2 Demonstra√ß√£o Pr√°tica (Ao Vivo)

**Setup recomendado:**
```bash
# Terminal 1: Executar backend
cd backend
python main.py
# Servidor rodando em http://localhost:8000

# Terminal 2: Servir frontend
python -m http.server 8080
# Interface em http://localhost:8080
```

**Perguntas para demonstrar:**

1. **Pergunta simples (fato)**
   - "Quais s√£o os sintomas da dengue?"
   - Esperado: Resposta clara e estruturada

2. **Pergunta de preven√ß√£o**
   - "Como prevenir mal√°ria em viagens para regi√£o end√™mica?"
   - Esperado: Recomenda√ß√µes pr√°ticas

3. **Pergunta sobre pol√≠tica p√∫blica**
   - "O que √© vacina√ß√£o em massa?"
   - Esperado: Explica√ß√£o de conceito

4. **Pergunta complexa**
   - "Qual a rela√ß√£o entre saneamento b√°sico e doen√ßas infecciosas?"
   - Esperado: Racioc√≠nio causal

---

### 8.3 M√©tricas para Apresentar

```python
# Gerar relat√≥rio de performance
def gerar_relatorio():
    return {
        "tamanho_dataset_treinamento": 1250,
        "tamanho_dataset_teste": 150,
        "tempo_treinamento_total": "45 minutos",
        "tempo_medio_resposta": "3.2 segundos",
        "memoria_gpu_pico": "6.8 GB",
        "acuracia_teste": 0.847,  # Se tiver dataset rotulado
        "perplexidade": 12.5,
        "f1_score": 0.82,
        "modelo_base": "LLaMA 2 7B",
        "adaptadores_lora": "13M par√¢metros",
        "reducao_memoria": "95%",
        "speedup_treinamento": "10x"
    }
```

---

### 8.4 Visualiza√ß√µes Impactantes

**Gr√°fico 1: Complexidade de Attention**
```
Tamanho da sequ√™ncia vs opera√ß√µes:
- 100 tokens: 10K ops
- 500 tokens: 250K ops
- 1000 tokens: 1M ops
- 2000 tokens: 4M ops
```

**Gr√°fico 2: Fine-tuning Loss**
```
Epoch 1: Loss 2.8
Epoch 2: Loss 1.5
Epoch 3: Loss 0.9
```

**Gr√°fico 3: Compara√ß√£o LoRA vs Full Training**
```
                    LoRA        Full
Mem√≥ria (GB)        2.1         28.0
Tempo (min)         45          450
Acur√°cia            0.847       0.851
```

---

## 9. CRONOGRAMA DE DESENVOLVIMENTO

| Fase | Dura√ß√£o | Atividades |
|------|---------|-----------|
| **1. Setup** | 1 semana | Ambiente, requisitos, datasets |
| **2. Explora√ß√£o de dados** | 1 semana | An√°lise, limpeza, formata√ß√£o |
| **3. Implementa√ß√£o backend** | 1.5 semanas | FastAPI, endpoints, integra√ß√£o modelo |
| **4. Fine-tuning** | 1 semana | Treinamento, valida√ß√£o, otimiza√ß√£o |
| **5. Frontend** | 0.5 semana | Interface HTML/JS |
| **6. Testes e polimento** | 0.5 semana | Testes, documenta√ß√£o |
| **7. Prepara√ß√£o apresenta√ß√£o** | 1 semana | Slides, demo, an√°lise complexidade |

**Total: ~7 semanas**

---

## 10. POTENCIAIS DESAFIOS E SOLU√á√ïES

| Desafio | Probabilidade | Solu√ß√£o |
|--------|--------------|---------|
| GPU insuficiente | Alta | LoRA reduz 95% da mem√≥ria; considerar Google Colab |
| Dataset pequeno | M√©dia | Data augmentation, usar exemplos sint√©ticos |
| Resposta gen√©rica | Alta | Aumentar num_epochs, ajustar temperatura, usar RAG |
| Overfitting | M√©dia | Valida√ß√£o regular, early stopping, dropout |
| Tempo de treinamento | M√©dia | LoRA + gradient checkpointing |
| Alucina√ß√µes (respostas falsas) | Alta | Fine-tuning robusto, RLHF avan√ßado (se time) |
| Interface lenta | Baixa | Cache de modelo, quantiza√ß√£o, async processing |

---

## 11. REFER√äNCIAS BIBLIOGR√ÅFICAS

### Artigos Cient√≠ficos
[1] Vaswani, A., et al. (2017). "Attention is All You Need." NeurIPS 2017.  
[2] Leong, H. Y., et al. (2024). "Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation." arXiv:2409.09324.  
[3] Zhu, X., et al. (2025). "Advancing medical question answering with a knowledge embedding transformer." PLOS ONE.  
[4] Bui, N., et al. (2025). "Fine-tuning large language models for improved health information inquiries." ScienceDirect.  

### Documenta√ß√£o Oficial
- PyTorch: https://pytorch.org/
- Hugging Face Transformers: https://huggingface.co/transformers/
- PEFT (LoRA): https://github.com/huggingface/peft
- FastAPI: https://fastapi.tiangolo.com/

### Recursos Educacionais
- "LLMs from Scratch" (Raschka)
- Hugging Face NLP Course
- Jay Alammar's Blog (The Illustrated Transformer)

---

## 12. CONCLUS√ÉO

Este projeto oferece:

‚úÖ **Aprendizado pr√°tico** de NLP, deep learning e arquitetura de sistemas  
‚úÖ **Aplica√ß√£o real** em dom√≠nio cr√≠tico (sa√∫de p√∫blica)  
‚úÖ **Desafio t√©cnico apropriado** para PAA  
‚úÖ **Demonstra√ß√£o clara** de an√°lise de complexidade  
‚úÖ **Resultado tang√≠vel** (sistema funcional)  

Boa sorte com o projeto! üöÄ
